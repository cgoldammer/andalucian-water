{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/cg/Dropbox/code/Python/water/app/backend/water/scripts\n"
     ]
    }
   ],
   "source": [
    "# type: ignore\n",
    "\n",
    "import pandas as pd\n",
    "import water.utils.parse as p\n",
    "import os\n",
    "import numpy as np\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd_water = os.getenv(\"CD_WATER\")\n",
    "\n",
    "folder_data = f'{cd_water}/data'\n",
    "folder_base = f'{folder_data}/datasets'\n",
    "\n",
    "folder = '{folder_data}/input/reserva'\n",
    "folder_new = '{folder_data}/input/cleaned'\n",
    "folder_input = '{folder_data}/input/cleaned'\n",
    "folder_output = '{folder_data}/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/input/reserva/2024_02_08.pdf\n",
      "./data/input/reserva/2024_01_29.pdf\n",
      "./data/input/reserva/2024_01_28.pdf\n",
      "./data/input/reserva/2024_01_31.pdf\n",
      "./data/input/reserva/2024_01_30.pdf\n",
      "./data/input/reserva/2024_01_27.pdf\n",
      "./data/input/reserva/2024_02_03.pdf\n",
      "./data/input/reserva/2024_02_02.pdf\n",
      "./data/input/reserva/2024_02_01.pdf\n",
      "./data/input/reserva/2024_02_05.pdf\n",
      "./data/input/reserva/2024_02_04.pdf\n",
      "./data/input/reserva/2024_02_06.pdf\n",
      "./data/input/reserva/2024_02_07.pdf\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            # Copy the file to the folder_new\n",
    "            filename_new = folder_new + '/' + file\n",
    "            # Get the old file, expanding the sub-path\n",
    "            filename_old = os.path.join(root, file)\n",
    "            # Copy from old to new if new does not exist\n",
    "            if not os.path.exists(filename_new):\n",
    "                print(filename_old)\n",
    "                os.system('cp ' + filename_old + ' ' + filename_new)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 400: 2024_02_08.pdf / share completed: 0.0%\n",
      "50 of 400: 2023_12_19.pdf / share completed: 12.5%\n",
      "100 of 400: 2023_10_30.pdf / share completed: 25.0%\n",
      "150 of 400: 2023_09_10.pdf / share completed: 37.5%\n",
      "200 of 400: 2023_07_20.pdf / share completed: 50.0%\n",
      "250 of 400: 2023_05_31.pdf / share completed: 62.5%\n",
      "300 of 400: 2023_04_10.pdf / share completed: 75.0%\n",
      "350 of 400: 2023_02_19.pdf / share completed: 87.5%\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(p)\n",
    "\n",
    "filenames_raw = sorted(os.listdir(folder_input))\n",
    "# filenames = [f for f in filenames_raw if p.filter_weekly(f)]\n",
    "filenames = filenames_raw[-400:][::-1]\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    if i % 50 == 0:\n",
    "        print(f'{i} of {len(filenames)}: {filename} / share completed: {round(i/len(filenames)*100, 2)}%')\n",
    "    full_filename = os.path.join(folder_new, filename)\n",
    "    filename_output = filename.replace('pdf', 'csv')\n",
    "    filename_output = os.path.join(folder_output, filename_output)\n",
    "    if not os.path.exists(filename_output):\n",
    "        try:\n",
    "            tables = p.get_tables(full_filename)\n",
    "            df_raw = p.get_df(tables)\n",
    "            df = p.clean_df(df_raw)\n",
    "            df.to_csv(filename_output, index=False)\n",
    "        except Exception as e:\n",
    "            print(full_filename)\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42036"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filenames_all = sorted(os.listdir(folder_output))\n",
    "df_all = p.get_full_df(filenames_all)\n",
    "df_all.to_csv('./data/datasets/all_parsed.csv', index=False)\n",
    "df_all = p.correct_issues(df_all).sort_values(['ds', 'province', 'reservoir'])\n",
    "\n",
    "def remove_bad_rows(df):\n",
    "    df_all = p.add_cols(df)\n",
    "\n",
    "    df_all['date_lag'] = df_all.groupby(['province', 'reservoir'])['date'].shift(1)\n",
    "\n",
    "    cols = ['rainfallsince', 'stored_hm3', 'capacity_hm3']\n",
    "    for var in ['rainfallsince', 'stored_hm3']:\n",
    "        df_all[f'{var}_diff'] = df_all.groupby(['province', 'reservoir'])[var].diff()\n",
    "        df_all[f'{var}_diff_0'] = df_all[f'{var}_diff']\n",
    "        for lags in range(1, 10):\n",
    "            df_all[f'{var}_diff_{lags}'] = df_all.groupby(['province', 'reservoir'])[f'{var}_diff'].shift(lags)\n",
    "            \n",
    "    df_all['bad_data'] = df_all.rainfallsince_diff_0 < -10\n",
    "    df_all['bad_data_for_year'] = df_all.groupby(['province', 'reservoir', 'year_climatic'])['bad_data'].transform('any')\n",
    "    df_all['problem_month'] = df_all.month.isin([1, 10, 11, 12])\n",
    "    df_all['bad_data_and_month'] = df_all.bad_data & df_all.problem_month\n",
    "\n",
    "    df_all['stored_hm3_diff_relative'] = df_all.stored_hm3_diff / df_all.capacity_hm3\n",
    "    df_reg = df_all.query('~bad_data_and_month').query('rainfallsince_diff >=0').copy()\n",
    "\n",
    "    df_reg['bad_data_for_year'] = df_reg.groupby(['province', 'reservoir', 'year_climatic'])['bad_data'].transform('any')\n",
    "    df_reg = df_reg.sort_values(['province', 'reservoir', 'date'])\n",
    "    df_reg['date_diff'] = (df_reg.date - df_reg.date_lag).dt.days\n",
    "    df_reg[['province', 'reservoir', 'date']].head(10)\n",
    "\n",
    "    df_reg['bad_data_for_year'] = df_reg.groupby(['province', 'reservoir', 'year_climatic'])['bad_data'].transform('any')\n",
    "    assert df_reg.bad_data_for_year.sum() == 0\n",
    "\n",
    "    def add_lags(df, var_name, lags=np.arange(-5, 5), groups=['province', 'reservoir']):\n",
    "        for lag in lags:\n",
    "            df[f'{var_name}_lag_{lag}'] = df.groupby(groups)[var_name].shift(lag)\n",
    "            \n",
    "        lag_vars = [f'{var_name}_lag_{lag}' for lag in lags]\n",
    "        return df, lag_vars\n",
    "\n",
    "    df_reg['suspicious_storage'] = (np.abs(df_reg['stored_hm3_diff']) > 2) & (np.abs(df_reg['stored_hm3_diff_relative']) > 0.05)\n",
    "    df_reg['high_rain'] = (df_reg.rainfallsince_diff > 10) | (df_reg.rainfallsince_diff_1 > 10)\n",
    "    df_reg['bad_storage'] = df_reg.suspicious_storage & (~df_reg.high_rain)\n",
    "    df_reg, lag_vars = add_lags(df_reg, 'bad_storage')\n",
    "\n",
    "    df_reg['surrounding_bad_storage'] = df_reg[lag_vars].max(axis=1)\n",
    "    df_reg = df_reg.query('surrounding_bad_storage==0').copy()\n",
    "    return df_reg\n",
    "\n",
    "df_removed = remove_bad_rows(df_all).sort_values(['ds', 'province', 'reservoir'])\n",
    "\n",
    "df_removed = df_removed[df_all.columns].sort_values(['reservoir', 'ds'])\n",
    "\n",
    "len(df_removed), len(df_all)\n",
    "df_removed.to_csv(f'{folder_base}/all_parsed_cleaned.csv', index=False)\n",
    "len(df_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_monthly(df):\n",
    "    df_monthly = df[df.ds.str.slice(8, 10) == \"01\"].copy()\n",
    "    return df_monthly\n",
    "\n",
    "# Pick only the first day of the month from df_all\n",
    "df_monthly = pick_monthly(df_all)\n",
    "df_monthly_cleaned = pick_monthly(df_removed)\n",
    "\n",
    "df_monthly.to_csv(f'{folder_base}//monthly.csv', index=False)\n",
    "df_monthly_cleaned.to_csv(f'{folder_base}//monthly_cleaned.csv', index=False)\n",
    "\n",
    "\n",
    "num_tail = 60\n",
    "ds_recent = df_all.ds.unique()[-num_tail:]\n",
    "df_recent = df_all[df_all.ds.isin(ds_recent)].copy()\n",
    "df_recent_cleaned = df_removed[df_removed.ds.isin(ds_recent)].copy()\n",
    "\n",
    "df_recent.to_csv(f'{folder_base}/recent.csv', index=False)\n",
    "df_recent_cleaned.to_csv(f'{folder_base}//recent_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "BUCKET = 'andalucianwater'\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "filename_end = 'all_parsed_cleaned.csv'\n",
    "\n",
    "filename = f'{folder_base}/{filename_end}'\n",
    "folder_app = f'{cd_water}/app/backend/water/data_raw'\n",
    "\n",
    "shutil.copy(filename, f'{folder_app}/{filename_end}')\n",
    "folder_s3 = 'data/cleaned'\n",
    "with open(filename, 'rb') as f:\n",
    "    s3.upload_fileobj(f, BUCKET, f'{folder_s3}/{filename_end}')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_geo = f'{folder_data}/reservoirs/InfGeografica/InfVectorial/Gpkg/Inv_presas_embalses.gpkg'\n",
    "folder_geo_s3 = 'data/raw/reservoirs_geo'\n",
    "with open(file_geo, 'rb') as f:\n",
    "    s3.upload_fileobj(f, BUCKET, f'{folder_geo_s3}/reservoirs.gpkg')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
