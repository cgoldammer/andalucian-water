{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import parse as p\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = './data/input/reserva'\n",
    "folder_new = './data/input/cleaned'\n",
    "folder_input = './data/input/cleaned'\n",
    "folder_output = './data/output'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/input/reserva/2024_02_08.pdf\n",
      "./data/input/reserva/2024_01_29.pdf\n",
      "./data/input/reserva/2024_01_28.pdf\n",
      "./data/input/reserva/2024_01_31.pdf\n",
      "./data/input/reserva/2024_01_30.pdf\n",
      "./data/input/reserva/2024_01_27.pdf\n",
      "./data/input/reserva/2024_02_03.pdf\n",
      "./data/input/reserva/2024_02_02.pdf\n",
      "./data/input/reserva/2024_02_01.pdf\n",
      "./data/input/reserva/2024_02_05.pdf\n",
      "./data/input/reserva/2024_02_04.pdf\n",
      "./data/input/reserva/2024_02_06.pdf\n",
      "./data/input/reserva/2024_02_07.pdf\n"
     ]
    }
   ],
   "source": [
    "for root, dirs, files in os.walk(folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            # Copy the file to the folder_new\n",
    "            filename_new = folder_new + '/' + file\n",
    "            # Get the old file, expanding the sub-path\n",
    "            filename_old = os.path.join(root, file)\n",
    "            # Copy from old to new if new does not exist\n",
    "            if not os.path.exists(filename_new):\n",
    "                print(filename_old)\n",
    "                os.system('cp ' + filename_old + ' ' + filename_new)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 of 400: 2024_02_08.pdf / share completed: 0.0%\n",
      "50 of 400: 2023_12_19.pdf / share completed: 12.5%\n",
      "100 of 400: 2023_10_30.pdf / share completed: 25.0%\n",
      "150 of 400: 2023_09_10.pdf / share completed: 37.5%\n",
      "200 of 400: 2023_07_20.pdf / share completed: 50.0%\n",
      "250 of 400: 2023_05_31.pdf / share completed: 62.5%\n",
      "300 of 400: 2023_04_10.pdf / share completed: 75.0%\n",
      "350 of 400: 2023_02_19.pdf / share completed: 87.5%\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(p)\n",
    "\n",
    "filenames_raw = sorted(os.listdir(folder_input))\n",
    "# filenames = [f for f in filenames_raw if p.filter_weekly(f)]\n",
    "filenames = filenames_raw[-400:][::-1]\n",
    "\n",
    "for i, filename in enumerate(filenames):\n",
    "    if i % 50 == 0:\n",
    "        print(f'{i} of {len(filenames)}: {filename} / share completed: {round(i/len(filenames)*100, 2)}%')\n",
    "    full_filename = os.path.join(folder_new, filename)\n",
    "    filename_output = filename.replace('pdf', 'csv')\n",
    "    filename_output = os.path.join(folder_output, filename_output)\n",
    "    if not os.path.exists(filename_output):\n",
    "        try:\n",
    "            tables = p.get_tables(full_filename)\n",
    "            df_raw = p.get_df(tables)\n",
    "            df = p.clean_df(df_raw)\n",
    "            df.to_csv(filename_output, index=False)\n",
    "        except Exception as e:\n",
    "            print(full_filename)\n",
    "            print(e)\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "filenames_all = sorted(os.listdir(folder_output))\n",
    "df_all = p.get_full_df(filenames_all)\n",
    "df_all.to_csv('./data/datasets/all_parsed.csv', index=False)\n",
    "df_all = p.correct_issues(df_all)\n",
    "\n",
    "def remove_bad_rows(df):\n",
    "    df_all = p.add_cols(df)\n",
    "\n",
    "    df_all['date_lag'] = df_all.groupby(['province', 'reservoir'])['date'].shift(1)\n",
    "\n",
    "    cols = ['rainfallsince', 'stored_hm3', 'capacity_hm3']\n",
    "    for var in ['rainfallsince', 'stored_hm3']:\n",
    "        df_all[f'{var}_diff'] = df_all.groupby(['province', 'reservoir'])[var].diff()\n",
    "        df_all[f'{var}_diff_0'] = df_all[f'{var}_diff']\n",
    "        for lags in range(1, 10):\n",
    "            df_all[f'{var}_diff_{lags}'] = df_all.groupby(['province', 'reservoir'])[f'{var}_diff'].shift(lags)\n",
    "            \n",
    "    df_all['bad_data'] = df_all.rainfallsince_diff_0 < -10\n",
    "    df_all['bad_data_for_year'] = df_all.groupby(['province', 'reservoir', 'year_climatic'])['bad_data'].transform('any')\n",
    "    df_all['problem_month'] = df_all.month.isin([1, 10, 11, 12])\n",
    "    df_all['bad_data_and_month'] = df_all.bad_data & df_all.problem_month\n",
    "\n",
    "    df_all['stored_hm3_diff_relative'] = df_all.stored_hm3_diff / df_all.capacity_hm3\n",
    "    df_reg = df_all.query('~bad_data_and_month').query('rainfallsince_diff >=0').copy()\n",
    "\n",
    "    df_reg['bad_data_for_year'] = df_reg.groupby(['province', 'reservoir', 'year_climatic'])['bad_data'].transform('any')\n",
    "    df_reg = df_reg.sort_values(['province', 'reservoir', 'date'])\n",
    "    df_reg['date_diff'] = (df_reg.date - df_reg.date_lag).dt.days\n",
    "    df_reg[['province', 'reservoir', 'date']].head(10)\n",
    "\n",
    "    df_reg['bad_data_for_year'] = df_reg.groupby(['province', 'reservoir', 'year_climatic'])['bad_data'].transform('any')\n",
    "    assert df_reg.bad_data_for_year.sum() == 0\n",
    "\n",
    "    def add_lags(df, var_name, lags=np.arange(-5, 5), groups=['province', 'reservoir']):\n",
    "        for lag in lags:\n",
    "            df[f'{var_name}_lag_{lag}'] = df.groupby(groups)[var_name].shift(lag)\n",
    "            \n",
    "        lag_vars = [f'{var_name}_lag_{lag}' for lag in lags]\n",
    "        return df, lag_vars\n",
    "\n",
    "    df_reg['suspicious_storage'] = (np.abs(df_reg['stored_hm3_diff']) > 2) & (np.abs(df_reg['stored_hm3_diff_relative']) > 0.05)\n",
    "    df_reg['high_rain'] = (df_reg.rainfallsince_diff > 10) | (df_reg.rainfallsince_diff_1 > 10)\n",
    "    df_reg['bad_storage'] = df_reg.suspicious_storage & (~df_reg.high_rain)\n",
    "    df_reg, lag_vars = add_lags(df_reg, 'bad_storage')\n",
    "\n",
    "    df_reg['surrounding_bad_storage'] = df_reg[lag_vars].max(axis=1)\n",
    "    df_reg = df_reg.query('surrounding_bad_storage==0').copy()\n",
    "    return df_reg\n",
    "\n",
    "df_removed = remove_bad_rows(df_all)\n",
    "df_removed = df_removed[df_all.columns]\n",
    "\n",
    "len(df_removed), len(df_all)\n",
    "df_all.to_csv('./data/datasets/all_parsed_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_monthly(df):\n",
    "    df_monthly = df[df.ds.str.slice(8, 10) == \"01\"].copy()\n",
    "    return df_monthly\n",
    "\n",
    "# Pick only the first day of the month from df_all\n",
    "df_monthly = pick_monthly(df_all)\n",
    "df_monthly_cleaned = pick_monthly(df_removed)\n",
    "\n",
    "df_monthly.to_csv('./data/datasets/monthly.csv', index=False)\n",
    "df_monthly_cleaned.to_csv('./data/datasets/monthly_cleaned.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
